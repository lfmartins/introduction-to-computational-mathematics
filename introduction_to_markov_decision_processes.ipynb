{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNIsTLxwcgdBgHwCAFp+7ng",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lfmartins/introduction-to-computational-mathematics/blob/main/introduction_to_markov_decision_processes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As an example of MDP, let's consider a robot that moves in a $4\\times4$ grid. The objective is to mimimize the number of steps the robot takes to reach the one of the target cells $(0,0)$ and $(3,3)$. \n",
        "\n",
        "If the robot is in grid cell $(i,j)$, there are at most 4 actions, corresponding to the robot moving north, east, south and west (if the robot is on one of the edges not all actions are allowed). The reward of taking any one of the actions is $-1$, except at the target cells, in which case the reward is zero. \n",
        "\n",
        "A discrete MDP is defined in terms of a function:\n",
        "\n",
        "$p(s', r \\mid s, a)$ is the probability that the process will be in state $s'$ at time $t$, given that the process is in sate $s$ at time $t-1$ and action $a$ is selected. (The notation $p(s', r \\mid s,a)$ is used to resemble the notation for conditional probability, but it just represents a function of three variables, $s'$, $s$ and $a$.\n",
        "\n",
        "A *randomized policy* is represented by function of two variables $\\pi(a|s)$ that, to every action $a$ available in state $s$, associates the probability that action $a$ is chosen when in state $s$.\n",
        "\n",
        "To represent a MDP in Python we define the following data structures.\n",
        "\n",
        "A MDP is represented by objects of the class `MDP`. A `MDP` holds information about a set of states. States are created by the method `add_state()`. When a state is added, the only information that is recorded is the `state_id`. This means that just adding the states does not define the structure of the chain.\n",
        "\n",
        "To each state, we associate a set of actions through the `add_action()` method. An action specifies a probability distribution on the set $\\mathscr{S}\\times\\mathscr{R}$.\n",
        "\n",
        "This version has no error checking, but it should eventually be added."
      ],
      "metadata": {
        "id": "ekGXJ8ufNLFi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MDP(object):\n",
        "  def __init__(self):\n",
        "    self.states = dict()\n",
        "  \n",
        "  def add_state(self, state_id):\n",
        "    self.states[state_id] = dict()\n",
        "\n",
        "  def add_action(self, state_id, action_id, probs):\n",
        "    self.states[state_id][action_id] = probs"
      ],
      "metadata": {
        "id": "40P5MICVh9RI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As an example, let's implement a MDP corresponding to the gridworld example."
      ],
      "metadata": {
        "id": "C9o4tXKVj_bS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use the following MDP as a test case. The state space is $\\mathscr{S}=\\{1,2,3,4\\}$. In each state, there are two possible actions, labeled $A$ and $B$. The following tables specify the transition probabilities and rewards for each action:\n",
        "\n"
      ],
      "metadata": {
        "id": "ZQdsbm0W_mqc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mdp = MDP()\n",
        "mdp.add_state(1)\n",
        "mdp.add_state(2)\n",
        "mdp.add_state(3)\n",
        "mdp.add_state(4)"
      ],
      "metadata": {
        "id": "LgPBHIJ6kjdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now add the actions:"
      ],
      "metadata": {
        "id": "80fddU44Mhp5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mdp.add_action(1, 'A', {(1, 1): 1/3, (2, 1): 2/3})\n",
        "mdp.add_action(1, 'B', {(2, 1): 1/2, (4, 1): 1/2})\n",
        "mdp.add_action(2, 'A', {(2, 2): 1/3, (3, 1): 2/3})\n",
        "mdp.add_action(2, 'B', {(1, 1): 1/2, (3, 1): 1/2})\n",
        "mdp.add_action(3, 'A', {(3, 3): 1/3, (3, 4): 2/3})\n",
        "mdp.add_action(3, 'B', {(2, 1): 1/2, (4, 1): 1/2})\n",
        "mdp.add_action(4, 'A', {(4, 4): 1/3, (1, 1): 2/3})\n",
        "mdp.add_action(4, 'B', {(1, 1): 1/2, (3, 1): 1/2})"
      ],
      "metadata": {
        "id": "qTNDjPhPkjQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "According to this setup, the value of $P(s',r \\mid s, a)$ is accessed in the `MDP` object as:\n",
        "\n",
        "    self.states[s][a][s', a]"
      ],
      "metadata": {
        "id": "4wRlWaVIO7rn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mdp.states[1]['A'][1, 1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8IxTN8iO5TY",
        "outputId": "91893a85-95dc-472d-f6ed-3a53428e642e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3333333333333333"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A policy is represented by a dictionary associating to each state a probability distribution"
      ],
      "metadata": {
        "id": "LPGmzBbGPoqJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "policy = {\n",
        "    1: {'A': 1/2, 'B': 1/2},\n",
        "    2: {'A': 1/4, 'B': 3/4},\n",
        "    3: {'A': 2/3, 'B': 1/3},\n",
        "    4: {'A':   0, 'B': 1}\n",
        "}"
      ],
      "metadata": {
        "id": "N57RpXNi_lPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "$p(a\\mid s)$ is represented by `policy[s][a]`\n",
        "\n",
        "We now need the code to solve Bellman's equation."
      ],
      "metadata": {
        "id": "Doezi4-GQSD0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NUj2Ceeo_lMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W9rmVPB8_lJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c96F0Dje_lHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uPLFjof4_lDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sikkiui7_lA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VqdzwgBh_k-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "26nzKvvt_k7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DbcsNIlQ_k4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fRDT-s1I_k2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VF4Q-h43_kzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "psYKvjSV_kw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = 4  # Grid size\n",
        "# We initialize the grid to \"empty\" cells\n",
        "states = [[None for j in range(n)] for i in range(n)]\n",
        "states"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xcNvVVkMdGl",
        "outputId": "6bfff656-fac9-41b1-8688-139b0f40be8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[None, None, None, None],\n",
              " [None, None, None, None],\n",
              " [None, None, None, None],\n",
              " [None, None, None, None]]"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inner cells\n",
        "for i in range(1, n-1):\n",
        "  for j in range(1, n-1):\n",
        "    states[i][j] = {\n",
        "        'N': {'reward': -1, 'tprobs': {(i-1, j): 1, (i-1, j): 0, (i, j+1): 0, (i+1, j): 0}},\n",
        "        'W': {'reward': -1, 'tprobs': {(i-1, j): 0, (i-1, j): 1, (i, j+1): 0, (i+1, j): 0}},\n",
        "        'S': {'reward': -1, 'tprobs': {(i-1, j): 0, (i-1, j): 0, (i, j+1): 1, (i+1, j): 0}},\n",
        "        'E': {'reward': -1, 'tprobs': {(i-1, j): 0, (i-1, j): 0, (i, j+1): 0, (i+1, j): 1}},\n",
        "    }\n",
        "# Edge cells, not corners\n",
        "for j in range(1, n-1):\n",
        "  # North edge\n",
        "  states[0][j] = {\n",
        "      'W': {'reward': -1, 'tprobs': {(i-1, j): 0, (i-1, j): 1, (i, j+1): 0, (i+1, j): 0}},\n",
        "      'S': {'reward': -1, 'tprobs': {(i-1, j): 0, (i-1, j): 0, (i, j+1): 1, (i+1, j): 0}},\n",
        "      'E': {'reward': -1, 'tprobs': {(i-1, j): 0, (i-1, j): 0, (i, j+1): 0, (i+1, j): 1}},\n",
        "  }\n",
        "  # West edge\n",
        "  states[j][0] = {\n",
        "      'N': {'reward': -1, 'tprobs': {(i-1, j): 1, (i-1, j): 0, (i, j+1): 0, (i+1, j): 0}},\n",
        "      'W': {'reward': -1, 'tprobs': {(i-1, j): 0, (i-1, j): 1, (i, j+1): 0, (i+1, j): 0}},\n",
        "      'S': {'reward': -1, 'tprobs': {(i-1, j): 0, (i-1, j): 0, (i, j+1): 1, (i+1, j): 0}},\n",
        "  }\n",
        "  # South edge \n",
        "  states[n-1][j] = {\n",
        "      'N': {'reward': -1, 'tprobs': {(i-1, j): 1, (i-1, j): 0, (i, j+1): 0, (i+1, j): 0}},\n",
        "      'W': {'reward': -1, 'tprobs': {(i-1, j): 0, (i-1, j): 1, (i, j+1): 0, (i+1, j): 0}},\n",
        "      'E': {'reward': -1, 'tprobs': {(i-1, j): 0, (i-1, j): 0, (i, j+1): 0, (i+1, j): 1}},\n",
        "  }\n",
        "  # East edge\n",
        "  states[j][n-1] = {\n",
        "      'N': {'reward': -1, 'tprobs': {(i-1, j): 1, (i-1, j): 0, (i, j+1): 0, (i+1, j): 0}},\n",
        "      'W': {'reward': -1, 'tprobs': {(i-1, j): 0, (i-1, j): 1, (i, j+1): 0, (i+1, j): 0}},\n",
        "      'S': {'reward': -1, 'tprobs': {(i-1, j): 0, (i-1, j): 0, (i, j+1): 1, (i+1, j): 0}},\n",
        "  }\n",
        "# Nortwest corner (terminal state)\n",
        "states[0][0] = {\n",
        "    'T': {'reward': 0, 'tprobs': {(0,0): 1}}\n",
        "}\n",
        "# Southwest corner\n",
        "states[n-1][0] = {\n",
        "    'N': {'reward': -1, 'tprobs': {(i-1, j): 1, (i-1, j): 0, (i, j+1): 0, (i+1, j): 0}},\n",
        "    'E': {'reward': -1, 'tprobs': {(i-1, j): 0, (i-1, j): 0, (i, j+1): 0, (i+1, j): 1}},\n",
        "}\n",
        "# Southeast corner (terminal state)\n",
        "states[n-1][n-1] = {\n",
        "    'T': {'reward': 0, 'tprobs': {(0,0): 1}}\n",
        "}\n",
        "# Northeast corner \n",
        "states[0][n-1] = {\n",
        "    'W': {'reward': -1, 'tprobs': {(i-1, j): 0, (i-1, j): 1, (i, j+1): 0, (i+1, j):0}},\n",
        "    'S': {'reward': -1, 'tprobs': {(i-1, j): 0, (i-1, j): 0, (i, j+1): 1, (i+1, j):0}},\n",
        "}"
      ],
      "metadata": {
        "id": "xhPFuu6ZAaAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A randomized policy $\\pi$ associates to each state a probability distribution on the set of all actions available at that state. Let's first consider the policy that assumes all actions at each state are equally likely:"
      ],
      "metadata": {
        "id": "nzInxVuTeMkl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "policy = [[None for j in range(n)] for i in range(n)]\n",
        "# Inner cells\n",
        "for i in range(0, n):\n",
        "  for j in range(0,n):\n",
        "    p = 1 / len(states[i][j])\n",
        "    policy[i][j] = dict()\n",
        "    for key in states[i][j]:\n",
        "      policy[i][j][key] = p"
      ],
      "metadata": {
        "id": "OKl_3uRIKRwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now compute the value $V_{\\pi}$ of this policy. We first derive a mathematical equation and then show how to implement its solution in Python. Suppose that the process currently is at state $s$. We then choose an action according to the probability distribution $\\pi(\\cdot|s)$. Let $a$ be the resulting action. Then, we receive a reward $r(s,a)$. So, the expected immediate reward when in state $s$ is:\n",
        "$$\n",
        "\\sum_{a\\in A(s)}\\pi(a|s)r(s,a)\n",
        "$$\n",
        "Then the process transitions to a new state according to the probability distribution $P(\\cdot|s,a)$. The expected future cost is:\n",
        "$$\n",
        "\\sum_{a\\in A(s)}\\pi(a|s)\\sum_{u}P(u|s,a)V_{\\pi}(u)\n",
        "$$\n",
        "We conclude that the system equations for $V_{\\pi}(\\cdot)$ is:\n",
        "$$\n",
        "V_{\\pi}(s) = \\sum_{a \\in A(s)}\\pi(a|s)\\left[r(s,a)+\\sum_{u}P(u|s,a)V_{\\pi}(u)\\right]\n",
        "$$\n",
        "Notice that this is a linear system on the unknown value function $V_{\\pi}(\\cdot)$. Instead of using a standard method (such as Gaussian Elimination), it use use an interactive method to compute the solution. We use the Gauss-Seidel method, which has a particularly simple implementation in this case.\n",
        "\n",
        "We choose an initial approximation for the value function, $V_{\\pi}(s)$, by choosing random values or simply using zeros. Then, we iterate the formula for $V_{\\pi}$ given above. This is implemented in the following code. \n"
      ],
      "metadata": {
        "id": "fijFZu4cgj6D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "niter = 100\n",
        "vfunc = [[0.0 for j in range(n)] for i in range(n)]\n",
        "for _ in range(niter):\n",
        "  for i in range(n):\n",
        "    for j in range(n):\n",
        "      for a, p in policy[i][j].items():\n",
        "        acc = states[i][j][a]['reward']\n",
        "        for s, q in states[i][j][a]['tprobs'].items():\n",
        "          k, l = s\n",
        "          acc += q * vfunc[k][l]\n",
        "        vfunc[i][j] = p * acc"
      ],
      "metadata": {
        "id": "tM5EnnFtfSBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(n):\n",
        "  for j in range(n):\n",
        "    print(f\"{vfunc[i][j]:8.5}\", end=' ')\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOYe_FHUp7bc",
        "outputId": "61f03dec-6844-4993-a106-d2893d82934b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     0.0 -0.49417 -0.49417 -0.74126 \n",
            "-0.45688 -0.34266 -0.34266 -0.45688 \n",
            "-0.49417 -0.37063 -0.37063 -0.48252 \n",
            "-0.74126 -0.48252 -0.48252      0.0 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for key, item in d.items():\n",
        "  print(key, item)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "id": "Kb-OmOLAIWb3",
        "outputId": "7ad69cb3-9222-4435-efa4-d39dabd2a078"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-477afb0e742d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'd' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code is to check the result"
      ],
      "metadata": {
        "id": "aRCpAu_APbjw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = 4\n",
        "v = [[0.0 for i in range(n)] for j in range(n)]\n",
        "for _ in range(30):\n",
        "  # Inner cells \n",
        "  for i in range(1, n-1):\n",
        "    for j in range(1, n-1):\n",
        "      v[i][j] = -1 + (1/4) * (v[i-1][j] + v[i][j-1] + v[i+1,j] + v[i][j+1])\n",
        "  # Top and bottom rows\n",
        "    for j in range(1, n-1):\n",
        "      v[j][0] = -1 + (1/3) * (v[])"
      ],
      "metadata": {
        "id": "V12p94E7IZ8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I1KVgWzuR52A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}